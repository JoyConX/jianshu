本文是[实施微服务，我们需要哪些基础框架？](https://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&mid=407641457&idx=1&sn=183d27056f3bd8ef17e77a3c15dfb3dd)的学习笔记

微服务架构一个Monolithic架构拆成了很多小的服务模块，那么第一个需要的东西很容易想到就是服务注册和发现功能，总的来说，常见有以下3种方案：

# 集中式LB方案

-------------------------------------------------------------------------------

服务消费者和服务提供者之间有一个独立的LB，LB通常是专门的硬件设备如F5，或者基于软件如LVS，HAproxy等实现。LB上有所有服务的地址映射表，通常由运维配置注册，当服务消费方调用某个目标服务时，它向LB发起请求，由LB以某种策略（比如Round-Robin）做负载均衡后将请求转发到目标服务。LB一般具备健康检查能力，能自动摘除不健康的服务实例。服务消费方如何发现LB呢？通常的做法是通过DNS，运维人员为服务配置一个DNS域名，这个域名指向LB。


![Fig 1， 集中式LB方案](http://upload-images.jianshu.io/upload_images/1435486-817ffb251c9087b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

集中式LB方案实现简单，在LB上也容易做集中式的访问控制，这一方案目前还是业界主流。集中式LB的主要问题是单点问题，所有服务调用流量都经过LB，当服务数量和调用量大的时候，LB容易成为瓶颈，且一旦LB发生故障对整个系统的影响是灾难性的。另外，LB在服务消费方和服务提供方之间增加了一跳(hop)，有一定性能开销。


# 进程内LB方案

-------------------------------------------------------------------------------

针对集中式LB的不足，进程内LB方案将LB的功能以库的形式集成到服务消费方进程里头，该方案也被称为软负载(Soft Load Balancing)或者客户端负载方案，下图Fig 2展示了这种方案的工作原理。这一方案需要一个服务注册表(Service Registry)配合支持服务自注册和自发现，服务提供方启动时，首先将服务地址注册到服务注册表（同时定期报心跳到服务注册表以表明服务的存活状态，相当于健康检查），服务消费方要访问某个服务时，它通过内置的LB组件向服务注册表查询（同时缓存并定期刷新）目标服务地址列表，然后以某种负载均衡策略选择一个目标服务地址，最后向目标服务发起请求。这一方案对服务注册表的可用性(Availability)要求很高，一般采用能满足高可用分布式一致的组件（例如Zookeeper， Consul， Etcd等）来实现。


![Fig 2 进程内LB方案](http://upload-images.jianshu.io/upload_images/1435486-4630fcede18764e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


进程内LB方案是一种分布式方案，LB和服务发现能力被分散到每一个服务消费者的进程内部，同时服务消费方和服务提供方之间是直接调用，没有额外开销，性能比较好。但是，该方案以客户库(Client Library)的方式集成到服务调用方进程里头，如果企业内有多种不同的语言栈，就要配合开发多种不同的客户端，有一定的研发和维护成本。另外，一旦客户端跟随服务调用方发布到生产环境中，后续如果要对客户库进行升级，势必要求服务调用方修改代码并重新发布，所以该方案的升级推广有不小的阻力。

进程内LB的案例是Netflix的开源服务框架，对应的组件分别是：Eureka服务注册表，Karyon服务端框架支持服务自注册和健康检查，Ribbon客户端框架支持服务自发现和软路由。另外，阿里开源的服务框架Dubbo也是采用类似机制。

# 主机独立LB进程方案

-------------------------------------------------------------------------------

该方案是针对第二种方案的不足而提出的一种折中方案，原理和第二种方案基本类似，不同之处是，他将LB和服务发现功能从进程内移出来，变成主机上的一个独立进程，主机上的一个或者多个服务要访问目标服务时，他们都通过同一主机上的独立LB进程做服务发现和负载均衡，见下图Fig 3。


![Fig 3 主机独立LB进程方案](http://upload-images.jianshu.io/upload_images/1435486-a82fd3d2fb26ff2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


该方案也是一种分布式方案，没有单点问题，一个LB进程挂了只影响该主机上的服务调用方，服务调用方和LB之间是进程内调用，性能好，同时，该方案还简化了服务调用方，不需要为不同语言开发客户库，LB的升级不需要服务调用方改代码。该方案的不足是部署较复杂，环节多，出错调试排查问题不方便。

该方案的典型案例是Airbnb的SmartStack服务发现框架，对应组件分别是：Zookeeper作为服务注册表，Nerve独立进程负责服务注册和健康检查，Synapse/HAproxy独立进程负责服务发现和负载均衡。Google最新推出的基于容器的PaaS平台Kubernetes，其内部服务发现采用类似的机制。

**总结**

> 方案1直接被否定，下面考虑方案2和方案3的实施。


# 基于zookeeper的名字发现服务

-------------------------------------------------------------------------------

## zookeeper介绍

ZooKeeper曾是Hadoop的正式子项目，后发展成为Apache顶级项目，与Hadoop密切相关但却没有任何依赖。它是一个针对大型应用提供高可用的数据管理、应用程序协调服务的分布式服务框架，基于对Paxos算法的实现，使该框架保证了分布式环境中数据的强一致性，提供的功能包括：配置维护、统一命名服务、状态同步服务、集群管理等。

## zookeeper的节点

zookeeper本上并不支持配置维护，统一命名服务等服务，它只支持节点创建管理，每一个节点可以有自己的数据，正常情况下一个zookeeper用起来会形成一棵节点树，比如:

```
/path/a
/path/a/room/1
/path/a/room/2

```
zookeeper节点的类型

+ Persistent Nodes: 永久有效地节点，除非client显式的删除，否则一直存在
+ Ephemeral Nodes: 临时节点，仅在创建该节点client保持连接期间有效，一旦连接丢失，zookeeper会自动删除该节点
+ Sequence Nodes: 顺序节点，client申请创建该节点时，zk会自动在节点路径末尾添加递增序号，这种类型是实现分布式锁，分布式queue等特殊功能的关键


## 如何创建服务

创建服务其实是通过创建zookeeper上的节点来完成的，可以假定某个平台的zookeeper路径如下：

```
/platform/
/platform/service_one
/platform/service_one/1   data -> {uri:"192.16.8.1"}
/platform/service_one/2   data -> {uri:"192.16.8.2"}
```


* 其中/platform/和/platform/service_one是Persistent Nodes，他由服务本上创建，永远不消失。
* data为节点数据，可以为空
* /platform/service_one/1和/platform/service_one/2为Ephemeral Nodes，它由对应的服务节点创建，当对应的服务宕机后(连接丢失)，该节点会被删除。

所以，每一个服务实例需要到自己的对应的zookeeper节点下注册自己为Ephemeral Node(临时节点)。

流程如下：

+ 服务启动后到固定zookeeper路径下建立一个新的临时节点。
+ 该临时节点可以间隔更新自己的uri信息和cpu内存等信息，方便客户端做负载均衡方案选择。
+ 如果服务不小心宕机，临时节点和zookeeper之间维护的心跳失败，那么该节点会被自动删除。


## 如何查询当前可以用的服务

作为服务的使用端，比如我需要service_one服务，通过zookeeper的api get_children(/platform/service_one）可以查询到service_one服务当前有n个服务实例。我随机选择一个，比如1-data-{uri:"192.16.8.1"}，通过拼接url来和节点通信。

`http://192.16.8.1/check?str=fwefwfe`

## zookeeper的watch功能

服务的使用端还需要监控自己使用的服务节点的变化，这可以使用zookeeper的watch功能完成，当使用端选择了(/platform/service_one/1)后，同时对其进行watch(/platform/service_one/1)。

watch某节点之后，如果zookeeper服务发现了次节点本身有任何变化，将会通知watch此节点的客户端，这样当/platform/service_one/1被删除后，客户端可以收到消息，然后重新选择一个可用节点。

通过zookeeper的watch功能还可以实现分布式配置发布管理。



# 学习Consul 

-------------------------------------------------------------------------------

+ [官网点我](https://www.Consul.io/)
+ [想快速浏览可以看我的学习笔记](http://www.jianshu.com/p/54011be847b2)
+ [Consul和ZooKeeper的区别](http://dockone.io/article/300)

Consul是由HashiCorp开发的服务发现和配置管理框架，和zookeeper在`服务发现`和`配置发布`这两个领域常常用来做对比，HashiCorp还有一个耳熟能详的工具:vagrant，是不是想说，“哦，原来是这个公司。”

Consul相比zookeeper主要有以下区别：

+ Consul是专门做服务注册和发现的/Zookeeper只是可以用来做服务注册和发现。
+ Consul用的是`主机独立的LB进程方案`，所以运维要麻烦一点/ Zookeeper用的是`进程内LB方案`方案，因此开发要麻烦一点，如上文所述。
+ Consul有漂亮的UI来做服务监控和节点监控/ Zookeeper没有，你需要DIY。
+ Consul支持Key/Value存储，可以用来做动态配置/Zookeeper支持节点数据，也可以用来做动态配置。
+ Consul支持多数据中心(多机房数据备份)/Zookeeper不支持。


总的来说Consul是专业做`服务发现`，还有UI管理，比Zookeeper要更加`拿来可用`，只是需要在每台机器上配置Consule-Agent，对服务本身不需要编写任何的代码来做服务发布，简单好用。


# 学习Etcd

-------------------------------------------------------------------------------

+ [github](https://github.com/coreos/etcd)
+ [etcd是什么东西？它和ZooKeeper有什么区别？](http://dockone.io/question/7)
+ [etcd：用于服务发现的键值存储系统](http://www.infoq.com/cn/news/2014/07/etcd-cluster-discovery)

etcd是一个高可用的键值存储系统，主要用于`共享配置和服务发现`。etcd是由CoreOS开发并维护的，灵感来自于 ZooKeeper 和 Doozer，它使用Go语言编写，并通过Raft一致性算法处理日志复制以保证强一致性。Raft是一个来自Stanford的新的一致性算法，适用于分布式系统的日志复制，Raft通过选举的方式来实现一致性，在Raft中，任何一个节点都可能成为Leader。Google的容器集群管理系统Kubernetes、开源PaaS平台Cloud Foundry和CoreOS的Fleet都广泛使用了etcd。

etcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。


Raft算法可以通过这个[动画](http://thesecretlivesofdata.com/raft/)来学习下，非常直观。




